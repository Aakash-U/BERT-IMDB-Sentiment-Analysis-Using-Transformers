{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4ee8142",
   "metadata": {},
   "source": [
    "# SentimentScope: IMDB Sentiment Analysis with Transformers\n",
    "\n",
    "A transformer-based sentiment analysis model trained from scratch on the IMDB movie review dataset. This project implements a custom GPT-style architecture for binary classification, achieving 76%+ accuracy on test data.\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "- **Task**: Binary sentiment classification (positive/negative)\n",
    "- **Dataset**: IMDB Movie Reviews (50,000 reviews)\n",
    "- **Architecture**: Custom transformer with 4 layers, 4 attention heads\n",
    "- **Performance**: 76.28% test accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0738efb7",
   "metadata": {},
   "source": [
    "## Data Loading and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4e8f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import urllib.request\n",
    "import tarfile\n",
    "\n",
    "if not os.path.exists('aclImdb'):\n",
    "    print(\"Downloading IMDB dataset...\")\n",
    "    url = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
    "    urllib.request.urlretrieve(url, 'aclImdb_v1.tar.gz')\n",
    "    \n",
    "    print(\"Extracting dataset...\")\n",
    "    with tarfile.open('aclImdb_v1.tar.gz', 'r:gz') as tar:\n",
    "        tar.extractall()\n",
    "    \n",
    "    print(\"Dataset ready!\")\n",
    "else:\n",
    "    print(\"Dataset already exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41a4caf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pos_path = 'aclImdb/train/pos'\n",
    "train_neg_path = 'aclImdb/train/neg'\n",
    "test_pos_path = 'aclImdb/test/pos'\n",
    "test_neg_path = 'aclImdb/test/neg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88450b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(folder):\n",
    "    reviews = []\n",
    "    for filename in os.listdir(folder):\n",
    "        file_path = os.path.join(folder, filename)\n",
    "        if filename.endswith(\".txt\"):\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                reviews.append(f.read())\n",
    "    return reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc1e6397",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pos = load_dataset(train_pos_path)\n",
    "train_neg = load_dataset(train_neg_path)\n",
    "test_pos = load_dataset(test_pos_path)\n",
    "test_neg = load_dataset(test_neg_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e3891dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              review  label\n",
      "0  Silly, hilarious, tragic, sad, inevitable.<br ...      1\n",
      "1  I actually like the original, and this film ha...      1\n",
      "2  For my humanities quarter project for school, ...      1\n",
      "3  To me this was more a wake up call, and realiz...      1\n",
      "4  This movie is a lot of fun. What makes it grea...      1\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.DataFrame({\n",
    "    'review': train_pos + train_neg,\n",
    "    'label': [1] * len(train_pos) + [0] * len(train_neg)\n",
    "})\n",
    "\n",
    "test_df = pd.DataFrame({\n",
    "    'review': test_pos + test_neg,\n",
    "    'label': [1] * len(test_pos) + [0] * len(test_neg)\n",
    "})\n",
    "\n",
    "print(train_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e696ead0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.9 * len(train_df))\n",
    "shuffled_df = train_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "train_data = shuffled_df.iloc[:train_size]\n",
    "val_data = shuffled_df.iloc[train_size:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f5c258",
   "metadata": {},
   "source": [
    "## Tokenization Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "10071ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2ca44c",
   "metadata": {},
   "source": [
    "## Custom Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6a9bb55a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "MAX_LENGTH = 128\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6976b570",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IMDBDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length=MAX_LENGTH):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.data.iloc[idx]['review']\n",
    "        label = self.data.iloc[idx]['label']\n",
    "\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        input_ids = encoding['input_ids'].squeeze()\n",
    "        attention_mask = encoding['attention_mask'].squeeze()\n",
    "\n",
    "        return input_ids, attention_mask, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a44edbf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = IMDBDataset(train_data, tokenizer)\n",
    "val_dataset = IMDBDataset(val_data, tokenizer)\n",
    "test_dataset = IMDBDataset(test_df, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bc21e582",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0b7cad",
   "metadata": {},
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "839a72c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"vocabulary_size\": tokenizer.vocab_size,\n",
    "    \"num_classes\": 2,\n",
    "    \"d_embed\": 128,\n",
    "    \"context_size\": MAX_LENGTH,\n",
    "    \"layers_num\": 4,\n",
    "    \"heads_num\": 4,\n",
    "    \"head_size\": 32,\n",
    "    \"dropout_rate\": 0.1,\n",
    "    \"use_bias\": True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "77c51747",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.Q_weights = nn.Linear(config[\"d_embed\"], config[\"head_size\"], bias=config[\"use_bias\"])\n",
    "        self.K_weights = nn.Linear(config[\"d_embed\"], config[\"head_size\"], bias=config[\"use_bias\"])\n",
    "        self.V_weights = nn.Linear(config[\"d_embed\"], config[\"head_size\"], bias=config[\"use_bias\"])\n",
    "        self.dropout = nn.Dropout(config[\"dropout_rate\"])\n",
    "\n",
    "        casual_attention_mask = torch.tril(torch.ones(config[\"context_size\"], config[\"context_size\"]))\n",
    "        self.register_buffer('casual_attention_mask', casual_attention_mask)\n",
    "\n",
    "    def forward(self, input):\n",
    "        batch_size, tokens_num, d_embed = input.shape\n",
    "        Q = self.Q_weights(input)\n",
    "        K = self.K_weights(input)\n",
    "        V = self.V_weights(input)\n",
    "\n",
    "        attention_scores = Q @ K.transpose(1, 2)\n",
    "        attention_scores = attention_scores.masked_fill(\n",
    "            self.casual_attention_mask[:tokens_num, :tokens_num] == 0,\n",
    "            float('-inf')\n",
    "        )\n",
    "        attention_scores = attention_scores / math.sqrt(K.shape[-1])\n",
    "        attention_scores = torch.softmax(attention_scores, dim=-1)\n",
    "        attention_scores = self.dropout(attention_scores)\n",
    "\n",
    "        return attention_scores @ V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b3411a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        heads_list = [AttentionHead(config) for _ in range(config[\"heads_num\"])]\n",
    "        self.heads = nn.ModuleList(heads_list)\n",
    "        self.linear = nn.Linear(config[\"heads_num\"] * config[\"head_size\"], config[\"d_embed\"])\n",
    "        self.dropout = nn.Dropout(config[\"dropout_rate\"])\n",
    "\n",
    "    def forward(self, input):\n",
    "        heads_outputs = [head(input) for head in self.heads]\n",
    "        x = torch.cat(heads_outputs, dim=-1)\n",
    "        x = self.linear(x)\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d5617365",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.linear_layers = nn.Sequential(\n",
    "            nn.Linear(config[\"d_embed\"], 4 * config[\"d_embed\"]),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * config[\"d_embed\"], config[\"d_embed\"]),\n",
    "            nn.Dropout(config[\"dropout_rate\"])\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.linear_layers(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "19fac2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.multi_head = MultiHeadAttention(config)\n",
    "        self.layer_norm_1 = nn.LayerNorm(config[\"d_embed\"])\n",
    "        self.feed_forward = FeedForward(config)\n",
    "        self.layer_norm_2 = nn.LayerNorm(config[\"d_embed\"])\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = input\n",
    "        x = x + self.multi_head(self.layer_norm_1(x))\n",
    "        x = x + self.feed_forward(self.layer_norm_2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "80a79d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DemoGPT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.token_embedding_layer = nn.Embedding(config[\"vocabulary_size\"], config[\"d_embed\"])\n",
    "        self.positional_embedding_layer = nn.Embedding(config[\"context_size\"], config[\"d_embed\"])\n",
    "        \n",
    "        blocks = [Block(config) for _ in range(config[\"layers_num\"])]\n",
    "        self.layers = nn.Sequential(*blocks)\n",
    "        \n",
    "        self.layer_norm = nn.LayerNorm(config[\"d_embed\"])\n",
    "        self.classifier = nn.Linear(config[\"d_embed\"], config[\"num_classes\"], bias=config[\"use_bias\"])\n",
    "        \n",
    "    def forward(self, token_ids):\n",
    "        batch_size, tokens_num = token_ids.shape\n",
    "\n",
    "        x = self.token_embedding_layer(token_ids)\n",
    "        positions = torch.arange(tokens_num, device=token_ids.device)\n",
    "        pos_embed = self.positional_embedding_layer(positions)\n",
    "        x = x + pos_embed.unsqueeze(0)\n",
    "        \n",
    "        x = self.layers(x)\n",
    "        x = self.layer_norm(x)\n",
    "        \n",
    "        x = torch.mean(x, dim=1)\n",
    "        logits = self.classifier(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40ef046",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "df0701b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(model, data_loader, device):\n",
    "    model.eval()\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            token_ids, attention_mask, labels = batch\n",
    "            token_ids = token_ids.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            logits = model(token_ids)\n",
    "            predictions = torch.argmax(logits, dim=1)\n",
    "\n",
    "            total_correct += (predictions == labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "    accuracy = (total_correct / total_samples) * 100\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "93b5a20a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3], Step [100/704], Loss: 0.6844\n",
      "Epoch [1/3], Step [200/704], Loss: 0.6625\n",
      "Epoch [1/3], Step [300/704], Loss: 0.6354\n",
      "Epoch [1/3], Step [400/704], Loss: 0.6052\n",
      "Epoch [1/3], Step [500/704], Loss: 0.6034\n",
      "Epoch [1/3], Step [600/704], Loss: 0.5757\n",
      "Epoch [1/3], Step [700/704], Loss: 0.5551\n",
      "Epoch 1 - Validation Accuracy: 66.48%\n",
      "Epoch [2/3], Step [100/704], Loss: 0.5232\n",
      "Epoch [2/3], Step [200/704], Loss: 0.5198\n",
      "Epoch [2/3], Step [300/704], Loss: 0.4939\n",
      "Epoch [2/3], Step [400/704], Loss: 0.4928\n",
      "Epoch [2/3], Step [500/704], Loss: 0.4971\n",
      "Epoch [2/3], Step [600/704], Loss: 0.4993\n",
      "Epoch [2/3], Step [700/704], Loss: 0.4706\n",
      "Epoch 2 - Validation Accuracy: 76.24%\n",
      "Epoch [3/3], Step [100/704], Loss: 0.4485\n",
      "Epoch [3/3], Step [200/704], Loss: 0.4402\n",
      "Epoch [3/3], Step [300/704], Loss: 0.4376\n",
      "Epoch [3/3], Step [400/704], Loss: 0.4331\n",
      "Epoch [3/3], Step [500/704], Loss: 0.4349\n",
      "Epoch [3/3], Step [600/704], Loss: 0.4305\n",
      "Epoch [3/3], Step [700/704], Loss: 0.4128\n",
      "Epoch 3 - Validation Accuracy: 77.12%\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "EPOCHS = 3\n",
    "\n",
    "model = DemoGPT(config).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=3e-4)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for step, (input_ids, attention_mask, labels) in enumerate(train_loader):\n",
    "        input_ids = input_ids.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        logits = model(input_ids)\n",
    "        loss = criterion(logits, labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "\n",
    "        if (step + 1) % 100 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{EPOCHS}], Step [{step+1}/{len(train_loader)}], \"\n",
    "                  f\"Loss: {running_loss/100:.4f}\")\n",
    "            running_loss = 0.0\n",
    "\n",
    "    val_accuracy = calculate_accuracy(model, val_loader, device)\n",
    "    print(f\"Epoch {epoch+1} - Validation Accuracy: {val_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d4fd56",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7d0167a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 76.27%\n"
     ]
    }
   ],
   "source": [
    "test_accuracy = calculate_accuracy(model, test_loader, device)\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "39b64c51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to: sentiment_model.pth\n"
     ]
    }
   ],
   "source": [
    "MODEL_PATH = \"sentiment_model.pth\"\n",
    "torch.save(model.state_dict(), MODEL_PATH)\n",
    "print(\"Model saved to:\", MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb72f72",
   "metadata": {},
   "source": [
    "## Results Summary\n",
    "\n",
    "### Performance Metrics\n",
    "- **Validation Accuracy**: 77.12%\n",
    "- **Test Accuracy**: 76.28%\n",
    "\n",
    "### Key Achievements\n",
    "- Successfully implemented a transformer architecture from scratch for sentiment classification\n",
    "- Achieved strong generalization with minimal overfitting (validation and test accuracy within 1%)\n",
    "- Efficient training with only 3 epochs\n",
    "\n",
    "### Technical Highlights\n",
    "- Custom GPT-style architecture with 4 transformer blocks\n",
    "- Multi-head attention mechanism (4 heads, 32-dimensional each)\n",
    "- Mean pooling for sequence-to-vector transformation\n",
    "- BERT tokenizer for subword tokenization\n",
    "\n",
    "### Future Improvements\n",
    "- Increase model capacity (more layers, larger embeddings)\n",
    "- Extended training with learning rate scheduling\n",
    "- Fine-tuning pretrained models (BERT, RoBERTa)\n",
    "- Ensemble methods for improved accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
